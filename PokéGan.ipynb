{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pok√©GAN\n",
    "This work is based on *forecore*'s work, avaliable on https://github.com/forcecore/Keras-GAN-Animeface-Character"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3.5/importlib/_bootstrap.py:222: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88\n",
      "  return f(*args, **kwds)\n",
      "/usr/lib/python3.5/importlib/_bootstrap.py:222: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88\n",
      "  return f(*args, **kwds)\n",
      "/usr/lib/python3.5/importlib/_bootstrap.py:222: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88\n",
      "  return f(*args, **kwds)\n"
     ]
    }
   ],
   "source": [
    "# Imports\n",
    "\n",
    "## Data import related imports\n",
    "import os\n",
    "import glob\n",
    "import h5py\n",
    "import numpy as np\n",
    "import scipy.misc\n",
    "import random\n",
    "import cv2\n",
    "\n",
    "import tensorflow as tf\n",
    "tf.logging.set_verbosity(tf.logging.ERROR)\n",
    "\n",
    "## Keras packages\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras.layers import Input, InputSpec, Layer, Dense, Activation, Flatten, Reshape, Dropout, BatchNormalization, Conv2D, Conv2DTranspose, UpSampling2D, LeakyReLU\n",
    "from tensorflow.keras import initializers, regularizers, constraints\n",
    "from tensorflow.keras import models\n",
    "from tensorflow.keras.optimizers import Adam, Adagrad, Adadelta, Adamax, SGD\n",
    "from tensorflow.keras.callbacks import CSVLogger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters and other important variables\n",
    "\n",
    "# Dataset size: use positive number to sample subset of the full dataset\n",
    "dataset_sz = -1\n",
    "\n",
    "# Archive outputs of training here for animating later.\n",
    "anim_dir = \"anim\"\n",
    "\n",
    "# Images size we will work on. (sz, sz, 4)\n",
    "sz = 64\n",
    "\n",
    "# Alpha, used by leaky relu of D and G networks.\n",
    "alpha_D = 0.2\n",
    "alpha_G = 0.2\n",
    "\n",
    "# Batch size, during training.\n",
    "batch_sz = 64\n",
    "\n",
    "# Length of the noise vector to generate the images from.\n",
    "# Latent space z\n",
    "noise_shape = (1, 1, 100)\n",
    "\n",
    "# GAN training can be ruined any moment if not careful.\n",
    "# Archive some snapshots in this directory.\n",
    "snapshot_dir = \"./snapshots\"\n",
    "\n",
    "# Dropout probability\n",
    "dropout = 0.3\n",
    "\n",
    "# Noisy label magnitude\n",
    "label_noise = 0.1\n",
    "\n",
    "# History to keep. Slower training but higher quality.\n",
    "history_sz = 12\n",
    "\n",
    "genw = \"gen.hdf5\"\n",
    "discw = \"disc.hdf5\"\n",
    "\n",
    "# Weight initialization function.\n",
    "#kernel_initializer = 'Orthogonal'\n",
    "#kernel_initializer = 'RandomNormal'\n",
    "# Same as default in Keras, but good for GAN, says\n",
    "# https://github.com/gheinrich/DIGITS-GAN/blob/master/examples/weight-init/README.md#experiments-with-lenet-on-mnist\n",
    "kernel_initializer = 'glorot_uniform'\n",
    "\n",
    "# DCGAN paper suggests 0.5.\n",
    "adam_beta = 0.5\n",
    "\n",
    "# BatchNormalization.\n",
    "bn_momentum = 0.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Functions used to generate h5 file containing data\n",
    "\n",
    "def normalize4gan(im):\n",
    "    '''\n",
    "    Convert colorspace and\n",
    "    cale the input in [-1, 1] range, as described in ganhacks\n",
    "    '''\n",
    "    im = im.astype(np.float32)\n",
    "    im /= 128.0\n",
    "    im -= 1.0 # now in [-1, 1]\n",
    "    return im\n",
    "\n",
    "def denormalize4gan(im):\n",
    "    '''\n",
    "    Does opposite of normalize4gan:\n",
    "    [-1, 1] to [0, 255].\n",
    "    Warning: input im is modified in-place!\n",
    "    '''\n",
    "    im += 1.0 # in [0, 2]\n",
    "    im *= 127.0 # in [0, 255]\n",
    "    return im.astype(np.uint8)\n",
    "\n",
    "def make_hdf5(ofname, wildcard):\n",
    "    '''\n",
    "    Preprocess files given by wildcard and save them in hdf5 file, as ofname.\n",
    "    '''\n",
    "    pool = list(glob.glob(wildcard))\n",
    "    if dataset_sz <= 0:\n",
    "        fnames = pool\n",
    "    else:\n",
    "        fnames = []\n",
    "        for i in range(dataset_sz):\n",
    "            # possible duplicate but don't care\n",
    "            fnames.append(random.choice(pool))\n",
    "\n",
    "    with h5py.File(ofname, \"w\") as f:\n",
    "        pokemons = f.create_dataset(\"pokemons\", (len(fnames), sz, sz, 4), dtype='f')\n",
    "\n",
    "        for i, fname in enumerate(fnames):\n",
    "            print(fname)\n",
    "            im = scipy.misc.imread(fname, mode='RGBA') # some have alpha channel\n",
    "            im = scipy.misc.imresize(im, (sz, sz))\n",
    "            pokemons[i] = normalize4gan(im)\n",
    "            \n",
    "def test(hdff):\n",
    "    '''\n",
    "    Reads in hdf file and check if pixels are scaled in [-1, 1] range.\n",
    "    '''\n",
    "    with h5py.File(hdff, \"r\") as f:\n",
    "        X = f.get(\"pokemons\")\n",
    "        print(np.min(X[:,:,:,0]))\n",
    "        print(np.max(X[:,:,:,0]))\n",
    "        print(np.min(X[:,:,:,1]))\n",
    "        print(np.max(X[:,:,:,1]))\n",
    "        print(np.min(X[:,:,:,2]))\n",
    "        print(np.max(X[:,:,:,2]))\n",
    "        print(\"Dataset size:\", len(X))\n",
    "        assert np.max(X) <= 1.0\n",
    "        assert np.min(X) >= -1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/git/PokeGAN/env/lib/python3.5/site-packages/ipykernel_launcher.py:41: DeprecationWarning: `imread` is deprecated!\n",
      "`imread` is deprecated in SciPy 1.0.0, and will be removed in 1.2.0.\n",
      "Use ``imageio.imread`` instead.\n",
      "/home/ubuntu/git/PokeGAN/env/lib/python3.5/site-packages/ipykernel_launcher.py:42: DeprecationWarning: `imresize` is deprecated!\n",
      "`imresize` is deprecated in SciPy 1.0.0, and will be removed in 1.2.0.\n",
      "Use ``skimage.transform.resize`` instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./training_data/718_f3.png\n",
      "./training_data/462.png\n",
      "./training_data/758.png\n",
      "./training_data/019.png\n",
      "./training_data/718.png\n",
      "./training_data/025.png\n",
      "./training_data/435.png\n",
      "./training_data/325.png\n",
      "./training_data/289.png\n",
      "./training_data/755.png\n",
      "./training_data/539.png\n",
      "./training_data/252.png\n",
      "./training_data/164.png\n",
      "./training_data/645.png\n",
      "./training_data/399.png\n",
      "./training_data/501.png\n",
      "./training_data/457.png\n",
      "./training_data/517.png\n",
      "./training_data/277.png\n",
      "./training_data/508.png\n",
      "-1.0\n",
      "0.9921875\n",
      "-1.0\n",
      "0.9921875\n",
      "-1.0\n",
      "0.9921875\n",
      "Dataset size: 20\n"
     ]
    }
   ],
   "source": [
    "# Create file, based on 'training_data' contents\n",
    "make_hdf5(\"data.hdf5\", \"./training_data/*.png\")\n",
    "\n",
    "# Check consistency of file\n",
    "test(\"data.hdf5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_____"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MinibatchDiscrimination(Layer):\n",
    "    \"\"\"Concatenates to each sample information about how different the input\n",
    "    features for that sample are from features of other samples in the same\n",
    "    minibatch, as described in Salimans et. al. (2016). Useful for preventing\n",
    "    GANs from collapsing to a single output. When using this layer, generated\n",
    "    samples and reference samples should be in separate batches.\n",
    "    # Example\n",
    "    ```python\n",
    "        # apply a convolution 1d of length 3 to a sequence with 10 timesteps,\n",
    "        # with 64 output filters\n",
    "        model = Sequential()\n",
    "        model.add(Convolution1D(64, 3, border_mode='same', input_shape=(10, 32)))\n",
    "        # now model.output_shape == (None, 10, 64)\n",
    "        # flatten the output so it can be fed into a minibatch discrimination layer\n",
    "        model.add(Flatten())\n",
    "        # now model.output_shape == (None, 640)\n",
    "        # add the minibatch discrimination layer\n",
    "        model.add(MinibatchDiscrimination(5, 3))\n",
    "        # now model.output_shape = (None, 645)\n",
    "    ```\n",
    "    # Arguments\n",
    "        nb_kernels: Number of discrimination kernels to use\n",
    "            (dimensionality concatenated to output).\n",
    "        kernel_dim: The dimensionality of the space where closeness of samples\n",
    "            is calculated.\n",
    "        init: name of initialization function for the weights of the layer\n",
    "            (see [initializations](../initializations.md)),\n",
    "            or alternatively, Theano function to use for weights initialization.\n",
    "            This parameter is only relevant if you don't pass a `weights` argument.\n",
    "        weights: list of numpy arrays to set as initial weights.\n",
    "        W_regularizer: instance of [WeightRegularizer](../regularizers.md)\n",
    "            (eg. L1 or L2 regularization), applied to the main weights matrix.\n",
    "        activity_regularizer: instance of [ActivityRegularizer](../regularizers.md),\n",
    "            applied to the network output.\n",
    "        W_constraint: instance of the [constraints](../constraints.md) module\n",
    "            (eg. maxnorm, nonneg), applied to the main weights matrix.\n",
    "        input_dim: Number of channels/dimensions in the input.\n",
    "            Either this argument or the keyword argument `input_shape`must be\n",
    "            provided when using this layer as the first layer in a model.\n",
    "    # Input shape\n",
    "        2D tensor with shape: `(samples, input_dim)`.\n",
    "    # Output shape\n",
    "        2D tensor with shape: `(samples, input_dim + nb_kernels)`.\n",
    "    # References\n",
    "        - [Improved Techniques for Training GANs](https://arxiv.org/abs/1606.03498)\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, nb_kernels, kernel_dim, init='glorot_uniform', weights=None,\n",
    "                 W_regularizer=None, activity_regularizer=None,\n",
    "                 W_constraint=None, input_dim=None, **kwargs):\n",
    "        self.init = initializers.get(init)\n",
    "        self.nb_kernels = nb_kernels\n",
    "        self.kernel_dim = kernel_dim\n",
    "        self.input_dim = input_dim\n",
    "\n",
    "        self.W_regularizer = regularizers.get(W_regularizer)\n",
    "        self.activity_regularizer = regularizers.get(activity_regularizer)\n",
    "\n",
    "        self.W_constraint = constraints.get(W_constraint)\n",
    "\n",
    "        self.initial_weights = weights\n",
    "        self.input_spec = [InputSpec(ndim=2)]\n",
    "\n",
    "        if self.input_dim:\n",
    "            kwargs['input_shape'] = (self.input_dim,)\n",
    "        super(MinibatchDiscrimination, self).__init__(**kwargs)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        assert len(input_shape) == 2\n",
    "\n",
    "        input_dim = input_shape[1]\n",
    "        self.input_spec = [InputSpec(dtype=K.floatx(),\n",
    "                                     shape=(None, input_dim))]\n",
    "\n",
    "        self.W = self.add_weight(shape=(self.nb_kernels, input_dim, self.kernel_dim),\n",
    "            initializer=self.init,\n",
    "            name='kernel',\n",
    "            regularizer=self.W_regularizer,\n",
    "            trainable=True,\n",
    "            constraint=self.W_constraint)\n",
    "\n",
    "        # Set built to true.\n",
    "        super(MinibatchDiscrimination, self).build(input_shape)\n",
    "\n",
    "    def call(self, x, mask=None):\n",
    "        activation = K.reshape(K.dot(x, self.W), (-1, self.nb_kernels, self.kernel_dim))\n",
    "        diffs = K.expand_dims(activation, 4) - K.expand_dims(K.permute_dimensions(activation, [1, 2, 0]), 0)\n",
    "        abs_diffs = K.sum(K.abs(diffs), axis=2)\n",
    "        minibatch_features = K.sum(K.exp(-abs_diffs), axis=2)\n",
    "        return K.concatenate([x, minibatch_features], 1)\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        assert input_shape and len(input_shape) == 2\n",
    "        return input_shape[0], input_shape[1]+self.nb_kernels\n",
    "\n",
    "    def get_config(self):\n",
    "        config = {'nb_kernels': self.nb_kernels,\n",
    "                  'kernel_dim': self.kernel_dim,\n",
    "                  'init': self.init.__name__,\n",
    "                  'W_regularizer': self.W_regularizer.get_config() if self.W_regularizer else None,\n",
    "                  'activity_regularizer': self.activity_regularizer.get_config() if self.activity_regularizer else None,\n",
    "                  'W_constraint': self.W_constraint.get_config() if self.W_constraint else None,\n",
    "                  'input_dim': self.input_dim}\n",
    "        base_config = super(MinibatchDiscrimination, self).get_config()\n",
    "        return dict(list(base_config.items()) + list(config.items()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Models\n",
    "\n",
    "def build_enc(shape) :\n",
    "    return build_discriminator(shape, build_disc=False)\n",
    "\n",
    "def build_discriminator( shape, build_disc=True ) :\n",
    "    '''\n",
    "    Build discriminator.\n",
    "    Set build_disc=False to build an encoder network to test\n",
    "    the encoding/discrimination capability with autoencoder...\n",
    "    '''\n",
    "    def conv2d( x, filters, shape=(4, 4), **kwargs ) :\n",
    "        x = Conv2D( filters, shape, strides=(2, 2),\n",
    "            padding='same',\n",
    "            kernel_initializer=kernel_initializer,\n",
    "            **kwargs )( x )\n",
    "        x = BatchNormalization(momentum=bn_momentum)( x )\n",
    "        x = LeakyReLU(alpha=alpha_D)( x )\n",
    "        return x\n",
    "\n",
    "    pokemon = Input( shape=shape )\n",
    "    x = pokemon\n",
    "\n",
    "    # Warning: Don't batchnorm the first set of Conv2D.\n",
    "    x = Conv2D( 64, (4, 4), strides=(2, 2),\n",
    "        padding='same',\n",
    "        kernel_initializer=kernel_initializer )( x )\n",
    "    x = LeakyReLU(alpha=alpha_D)( x )\n",
    "    # 32x32\n",
    "\n",
    "    x = conv2d( x, 128 )\n",
    "    # 16x16\n",
    "\n",
    "    x = conv2d( x, 256 )\n",
    "    # 8x8\n",
    "\n",
    "    x = conv2d( x, 512 )\n",
    "    # 4x4\n",
    "\n",
    "    if build_disc:\n",
    "        x = Flatten()(x)\n",
    "        # 1 when \"real\", 0 when \"fake\".\n",
    "        x = Dense(1, activation='sigmoid',\n",
    "            kernel_initializer=kernel_initializer)( x )\n",
    "        return models.Model( inputs=pokemon, outputs=x )\n",
    "    else:\n",
    "        # build encoder.\n",
    "        x = Conv2D(noise_shape[2], (4, 4), activation='tanh')(x)\n",
    "        return models.Model( inputs=pokemon, outputs=x )\n",
    "    \n",
    "def build_gen( shape ) :\n",
    "    def deconv2d( x, filters, shape=(4, 4) ) :\n",
    "        '''\n",
    "        Conv2DTransposed gives me checkerboard artifact...\n",
    "        Select one of the 3.\n",
    "        '''\n",
    "\n",
    "        x = Conv2DTranspose( filters, shape, padding='same',\n",
    "            strides=(2, 2), kernel_initializer=kernel_initializer )(x)\n",
    "\n",
    "        x = BatchNormalization(momentum=bn_momentum)( x )\n",
    "        x = LeakyReLU(alpha=alpha_G)( x )\n",
    "        return x\n",
    "\n",
    "    noise = Input( shape=noise_shape )\n",
    "    x = noise\n",
    "    # 1x1x256\n",
    "\n",
    "    x= Conv2DTranspose( 512, (4, 4),\n",
    "        kernel_initializer=kernel_initializer )(x)\n",
    "    x = BatchNormalization(momentum=bn_momentum)( x )\n",
    "    x = LeakyReLU(alpha=alpha_G)( x )\n",
    "    # 4x4\n",
    "    x = deconv2d( x, 256 )\n",
    "    # 8x8\n",
    "    x = deconv2d( x, 128 )\n",
    "    # 16x16\n",
    "    x = deconv2d( x, 64 )\n",
    "    # 32x32\n",
    "\n",
    "    # Extra layer\n",
    "    x = Conv2D( 64, (3, 3), padding='same',\n",
    "        kernel_initializer=kernel_initializer )( x )\n",
    "    x = BatchNormalization(momentum=bn_momentum)( x )\n",
    "    x = LeakyReLU(alpha=alpha_G)( x )\n",
    "    # 32x32\n",
    "\n",
    "    x= Conv2DTranspose( 4, (4, 4), padding='same', activation='tanh',\n",
    "        strides=(2, 2), kernel_initializer=kernel_initializer )(x)\n",
    "    # 64x64\n",
    "\n",
    "    return models.Model(inputs=noise, outputs=x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "____"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_pokemon( pokemons ):\n",
    "    reals = []\n",
    "    for i in range( batch_sz ) :\n",
    "        j = random.randrange( len(pokemons) )\n",
    "        pokemon = pokemons[ j ]\n",
    "        reals.append( pokemon )\n",
    "    reals = np.array(reals)\n",
    "    return reals\n",
    "\n",
    "def binary_noise(cnt):\n",
    "    # Note about noise range.\n",
    "    # 0, 1 noise vs -1, 1 noise. -1, 1 seems to be better and stable.\n",
    "\n",
    "    noise = label_noise * np.random.ranf((cnt,) + noise_shape) # [0, 0.1]\n",
    "    noise -= 0.05 # [-0.05, 0.05]\n",
    "    noise += np.random.randint(0, 2, size=((cnt,) + noise_shape))\n",
    "\n",
    "    noise -= 0.5\n",
    "    noise *= 2\n",
    "    return noise\n",
    "\n",
    "def sample_fake( gen ) :\n",
    "    noise = binary_noise(batch_sz)\n",
    "    fakes = gen.predict(noise)\n",
    "    return fakes, noise\n",
    "\n",
    "def dump_batch(imgs, cnt, ofname):\n",
    "    '''\n",
    "    Merges cnt x cnt generated images into one big image.\n",
    "    Use the command\n",
    "    $ feh dump.png --reload 1\n",
    "    to refresh image peroidically during training!\n",
    "    '''\n",
    "    assert batch_sz >= cnt * cnt\n",
    "\n",
    "    rows = []\n",
    "\n",
    "    for i in range( cnt ) :\n",
    "        cols = []\n",
    "        for j in range(cnt*i, cnt*i+cnt):\n",
    "            cols.append( imgs[j] )\n",
    "        rows.append( np.concatenate(cols, axis=1) )\n",
    "\n",
    "    alles = np.concatenate( rows, axis=0 )\n",
    "    alles = denormalize4gan( alles )\n",
    "    scipy.misc.imsave(ofname, alles)\n",
    "    \n",
    "def build_networks():\n",
    "    shape = (sz, sz, 4)\n",
    "\n",
    "    dopt = Adam(lr=0.0002, beta_1=adam_beta)\n",
    "    opt  = Adam(lr=0.0001, beta_1=adam_beta)\n",
    "\n",
    "    # generator part\n",
    "    gen = build_gen( shape )\n",
    "    # loss function doesn't seem to matter for this one, as it is not directly trained\n",
    "    gen.compile(optimizer=opt, loss='binary_crossentropy')\n",
    "    gen.summary()\n",
    "\n",
    "    # discriminator part\n",
    "    disc = build_discriminator( shape )\n",
    "    disc.compile(optimizer=dopt, loss='binary_crossentropy')\n",
    "    disc.summary()\n",
    "\n",
    "    # GAN stack\n",
    "    # https://ctmakro.github.io/site/on_learning/fast_gan_in_keras.html is the faster way.\n",
    "    # Here, for simplicity, I use slower way (slower due to duplicate computation).\n",
    "    noise = Input( shape=noise_shape )\n",
    "    gened = gen( noise )\n",
    "    result = disc( gened )\n",
    "    gan = models.Model( inputs=noise, outputs=result )\n",
    "    gan.compile(optimizer=opt, loss='binary_crossentropy')\n",
    "    gan.summary()\n",
    "\n",
    "    return gen, disc, gan\n",
    "\n",
    "def train_autoenc( dataf ):\n",
    "    '''\n",
    "    Train an autoencoder first to see if your network is large enough.\n",
    "    '''\n",
    "    f = h5py.File(dataf, 'r')\n",
    "    pokemons = f.get('pokemons')\n",
    "\n",
    "    opt = Adam(lr=0.001)\n",
    "\n",
    "    shape = (sz, sz, 4)\n",
    "    enc = build_enc( shape )\n",
    "    enc.compile(optimizer=opt, loss='mse')\n",
    "    enc.summary()\n",
    "\n",
    "    # generator part\n",
    "    gen = build_gen( shape )\n",
    "    # generator is not directly trained. Optimizer and loss doesn't matter too much.\n",
    "    gen.compile(optimizer=opt, loss='mse')\n",
    "    gen.summary()\n",
    "\n",
    "    pokemon = Input( shape=shape )\n",
    "    vector = enc(pokemon)\n",
    "    recons = gen(vector)\n",
    "    autoenc = models.Model( inputs=pokemon, outputs=recons )\n",
    "    autoenc.compile(optimizer=opt, loss='mse')\n",
    "\n",
    "    epoch = 0\n",
    "    while epoch < 200 :\n",
    "        for i in range(10) :\n",
    "            reals = sample_pokemon(pokemons)\n",
    "            fakes, noises = sample_fake( gen )\n",
    "            loss = autoenc.train_on_batch( reals, reals )\n",
    "            epoch += 1\n",
    "            print(epoch, loss)\n",
    "        fakes = autoenc.predict(reals)\n",
    "        dump_batch(fakes, 4, \"fakes.png\")\n",
    "        dump_batch(reals, 4, \"reals.png\")\n",
    "    gen.save_weights(genw)\n",
    "    enc.save_weights(discw)\n",
    "    print(\"Saved\", genw, discw)\n",
    "\n",
    "def load_weights(model, wf):\n",
    "    '''\n",
    "    I find error message in load_weights hard to understand sometimes.\n",
    "    '''\n",
    "    try:\n",
    "        model.load_weights(wf)\n",
    "    except:\n",
    "        print(\"failed to load weight, network changed or corrupt hdf5\", wf, file=sys.stderr)\n",
    "        sys.exit(1)\n",
    "        \n",
    "def train_gan( dataf ) :\n",
    "    gen, disc, gan = build_networks()\n",
    "\n",
    "    # Uncomment these, if you want to continue training from some snapshot.\n",
    "    # (or load pretrained generator weights)\n",
    "    #load_weights(gen, genw)\n",
    "    #load_weights(disc, discw)\n",
    "\n",
    "    logger = CSVLogger('loss.csv') # yeah, you can use callbacks independently\n",
    "    logger.on_train_begin() # initialize csv file\n",
    "    with h5py.File( dataf, 'r' ) as f :\n",
    "        pokemons = f.get( 'pokemons' )\n",
    "        run_batches(gen, disc, gan, pokemons, logger, range(5000))\n",
    "    logger.on_train_end()\n",
    "    \n",
    "def run_batches(gen, disc, gan, pokemons, logger, itr_generator):\n",
    "    history = [] # need this to prevent G from shifting from mode to mode to trick D.\n",
    "    train_disc = True\n",
    "    for batch in itr_generator:\n",
    "        # Using soft labels here.\n",
    "        lbl_fake = label_noise * np.random.ranf(batch_sz)\n",
    "        lbl_real = 1 - label_noise * np.random.ranf(batch_sz)\n",
    "\n",
    "        fakes, noises = sample_fake( gen )\n",
    "        reals = sample_pokemon( pokemons )\n",
    "        # Add noise...\n",
    "        # My dataset works without this.\n",
    "        #reals += 0.5 * np.exp(-batch/100) * np.random.normal( size=reals.shape )\n",
    "\n",
    "        if batch % 10 == 0 :\n",
    "            if len(history) > history_sz:\n",
    "                history.pop(0) # evict oldest\n",
    "            history.append( (reals, fakes) )\n",
    "\n",
    "        gen.trainable = False\n",
    "        #for reals, fakes in history:\n",
    "        d_loss1 = disc.train_on_batch( reals, lbl_real )\n",
    "        d_loss0 = disc.train_on_batch( fakes, lbl_fake )\n",
    "        gen.trainable = True\n",
    "       \n",
    "        #if d_loss1 > 15.0 or d_loss0 > 15.0 :\n",
    "        # artificial training of one of G or D based on\n",
    "        # statistics is not good at all.\n",
    "\n",
    "        # pretrain train discriminator only\n",
    "        if batch < 20 :\n",
    "            print( batch, \"d0:{} d1:{}\".format( d_loss0, d_loss1 ) )\n",
    "            continue\n",
    "\n",
    "        disc.trainable = False\n",
    "        g_loss = gan.train_on_batch( noises, lbl_real ) # try to trick the classifier.\n",
    "        disc.trainable = True\n",
    "\n",
    "        # To escape this loop, both D and G should be trained so that\n",
    "        # D begins to mark everything that's wrong that G has done.\n",
    "        # Otherwise G will only change locally and fail to escape the minima.\n",
    "        #train_disc = True if g_loss < 15 else False\n",
    "\n",
    "        print( batch, \"d0:{} d1:{}   g:{}\".format( d_loss0, d_loss1, g_loss ) )\n",
    "\n",
    "        # save weights every 10 batches\n",
    "        if batch % 10 == 0 and batch != 0 :\n",
    "            end_of_batch_task(batch, gen, disc, reals, fakes)\n",
    "            row = {\"d_loss0\": d_loss0, \"d_loss1\": d_loss1, \"g_loss\": g_loss}\n",
    "#             logger.on_epoch_end(batch, row)\n",
    "            \n",
    "_bits = binary_noise(batch_sz)\n",
    "def end_of_batch_task(batch, gen, disc, reals, fakes):\n",
    "    try :\n",
    "        # Dump how the generator is doing.\n",
    "        # Animation dump\n",
    "        dump_batch(reals, 4, \"reals.png\")\n",
    "        dump_batch(fakes, 4, \"fakes.png\") # to check how noisy the image is\n",
    "        frame = gen.predict(_bits)\n",
    "        animf = os.path.join(anim_dir, \"frame_{:08d}.png\".format(int(batch/10)))\n",
    "        dump_batch(frame, 4, animf)\n",
    "        dump_batch(frame, 4, \"frame.png\")\n",
    "\n",
    "        serial = int(batch / 10) % 10\n",
    "        prefix = os.path.join(snapshot_dir, str(serial) + \".\")\n",
    "\n",
    "        print(\"Saving weights\", serial)\n",
    "        gen.save_weights(prefix + genw)\n",
    "        disc.save_weights(prefix + discw)\n",
    "    except KeyboardInterrupt :\n",
    "        print(\"Saving, don't interrupt with Ctrl+C!\", serial)\n",
    "        # recursion to surely save everything haha\n",
    "        end_of_batch_task(batch, gen, disc, reals, fakes)\n",
    "        raise\n",
    "        \n",
    "def generate(genw,cnt):\n",
    "    shape = (sz, sz, 4)\n",
    "    gen = build_gen(shape)\n",
    "    gen.compile(optimizer='sgd', loss='mse')\n",
    "    load_weights(gen, genw)\n",
    "\n",
    "    generated = gen.predict(binary_noise(batch_sz))\n",
    "    # Unoffset, in batch.\n",
    "    # Must convert back to unit8 to stop color distortion.\n",
    "    generated = denormalize4gan(generated)\n",
    "\n",
    "    for i in range(cnt):\n",
    "        ofname = \"{:04d}.png\".format(i)\n",
    "        scipy.misc.imsave(ofname, generated[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         (None, 1, 1, 100)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_transpose (Conv2DTran (None, 4, 4, 512)         819712    \n",
      "_________________________________________________________________\n",
      "batch_normalization (BatchNo (None, 4, 4, 512)         2048      \n",
      "_________________________________________________________________\n",
      "leaky_re_lu (LeakyReLU)      (None, 4, 4, 512)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_transpose_1 (Conv2DTr (None, 8, 8, 256)         2097408   \n",
      "_________________________________________________________________\n",
      "batch_normalization_1 (Batch (None, 8, 8, 256)         1024      \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_1 (LeakyReLU)    (None, 8, 8, 256)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_transpose_2 (Conv2DTr (None, 16, 16, 128)       524416    \n",
      "_________________________________________________________________\n",
      "batch_normalization_2 (Batch (None, 16, 16, 128)       512       \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_2 (LeakyReLU)    (None, 16, 16, 128)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_transpose_3 (Conv2DTr (None, 32, 32, 64)        131136    \n",
      "_________________________________________________________________\n",
      "batch_normalization_3 (Batch (None, 32, 32, 64)        256       \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_3 (LeakyReLU)    (None, 32, 32, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d (Conv2D)              (None, 32, 32, 64)        36928     \n",
      "_________________________________________________________________\n",
      "batch_normalization_4 (Batch (None, 32, 32, 64)        256       \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_4 (LeakyReLU)    (None, 32, 32, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_transpose_4 (Conv2DTr (None, 64, 64, 4)         4100      \n",
      "=================================================================\n",
      "Total params: 3,617,796\n",
      "Trainable params: 3,615,748\n",
      "Non-trainable params: 2,048\n",
      "_________________________________________________________________\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_2 (InputLayer)         (None, 64, 64, 4)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_1 (Conv2D)            (None, 32, 32, 64)        4160      \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_5 (LeakyReLU)    (None, 32, 32, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 16, 16, 128)       131200    \n",
      "_________________________________________________________________\n",
      "batch_normalization_5 (Batch (None, 16, 16, 128)       512       \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_6 (LeakyReLU)    (None, 16, 16, 128)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            (None, 8, 8, 256)         524544    \n",
      "_________________________________________________________________\n",
      "batch_normalization_6 (Batch (None, 8, 8, 256)         1024      \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_7 (LeakyReLU)    (None, 8, 8, 256)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_4 (Conv2D)            (None, 4, 4, 512)         2097664   \n",
      "_________________________________________________________________\n",
      "batch_normalization_7 (Batch (None, 4, 4, 512)         2048      \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_8 (LeakyReLU)    (None, 4, 4, 512)         0         \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 8192)              0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 1)                 8193      \n",
      "=================================================================\n",
      "Total params: 2,769,345\n",
      "Trainable params: 2,767,553\n",
      "Non-trainable params: 1,792\n",
      "_________________________________________________________________\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_3 (InputLayer)         (None, 1, 1, 100)         0         \n",
      "_________________________________________________________________\n",
      "model (Model)                (None, 64, 64, 4)         3617796   \n",
      "_________________________________________________________________\n",
      "model_1 (Model)              (None, 1)                 2769345   \n",
      "=================================================================\n",
      "Total params: 6,387,141\n",
      "Trainable params: 6,383,301\n",
      "Non-trainable params: 3,840\n",
      "_________________________________________________________________\n",
      "0 d0:2.9840610027313232 d1:0.5123764276504517\n",
      "1 d0:0.33414390683174133 d1:0.5513021945953369\n",
      "2 d0:0.4909515082836151 d1:0.43306034803390503\n",
      "3 d0:0.39853203296661377 d1:0.33896878361701965\n",
      "4 d0:0.3286464810371399 d1:0.24230460822582245\n",
      "5 d0:0.34092313051223755 d1:0.2268531322479248\n",
      "6 d0:0.24044805765151978 d1:0.22025960683822632\n",
      "7 d0:0.227390855550766 d1:0.22227004170417786\n",
      "8 d0:0.2348668873310089 d1:0.19551560282707214\n",
      "9 d0:0.22073009610176086 d1:0.2321874052286148\n",
      "10 d0:0.2283608317375183 d1:0.29110410809516907\n",
      "11 d0:0.19631904363632202 d1:0.47084373235702515\n",
      "12 d0:0.2353326380252838 d1:0.5172655582427979\n",
      "13 d0:0.18580201268196106 d1:0.26037076115608215\n",
      "14 d0:0.2387048751115799 d1:0.31569617986679077\n",
      "15 d0:0.23264676332473755 d1:0.5557055473327637\n",
      "16 d0:0.2287052571773529 d1:0.44306784868240356\n",
      "17 d0:0.21667256951332092 d1:0.31065845489501953\n",
      "18 d0:0.24218614399433136 d1:0.3414231836795807\n",
      "19 d0:0.2228921353816986 d1:0.35422828793525696\n",
      "20 d0:0.2565193176269531 d1:0.32983601093292236   g:3.541607618331909\n",
      "Saving weights 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/git/PokeGAN/env/lib/python3.5/site-packages/ipykernel_launcher.py:46: DeprecationWarning: `imsave` is deprecated!\n",
      "`imsave` is deprecated in SciPy 1.0.0, and will be removed in 1.2.0.\n",
      "Use ``imageio.imwrite`` instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "21 d0:1.8590009212493896 d1:0.3443704843521118   g:6.792106628417969\n",
      "22 d0:0.3906201720237732 d1:0.7925941348075867   g:1.3193854093551636\n",
      "23 d0:3.641796827316284 d1:0.6691110730171204   g:9.67149543762207\n",
      "24 d0:0.5376254320144653 d1:0.5493295788764954   g:10.408368110656738\n",
      "25 d0:0.3834400177001953 d1:0.3876746594905853   g:5.999385833740234\n",
      "26 d0:0.4735194444656372 d1:0.3094353973865509   g:5.386129379272461\n",
      "27 d0:0.46801501512527466 d1:0.234207421541214   g:6.462993144989014\n",
      "28 d0:0.22067303955554962 d1:0.2752486765384674   g:3.839236259460449\n",
      "29 d0:0.9214929938316345 d1:0.31115788221359253   g:9.444129943847656\n",
      "30 d0:0.3560725450515747 d1:0.393726110458374   g:6.910520553588867\n",
      "Saving weights 3\n",
      "31 d0:0.2147250473499298 d1:0.3370794653892517   g:3.4323973655700684\n",
      "32 d0:1.3456591367721558 d1:0.39064645767211914   g:9.741103172302246\n",
      "33 d0:0.4307938814163208 d1:0.4005632996559143   g:8.491881370544434\n",
      "34 d0:0.2533150315284729 d1:0.260436475276947   g:4.456714153289795\n",
      "35 d0:0.6575123071670532 d1:0.35820263624191284   g:6.283242225646973\n",
      "36 d0:0.20943446457386017 d1:0.28107762336730957   g:4.68203592300415\n",
      "37 d0:0.3556596636772156 d1:0.23038014769554138   g:4.781714916229248\n",
      "38 d0:0.36254438757896423 d1:0.24332277476787567   g:5.445164680480957\n",
      "39 d0:0.24742180109024048 d1:0.2582244277000427   g:4.0372772216796875\n",
      "40 d0:0.5377638339996338 d1:0.23031949996948242   g:7.423376083374023\n",
      "Saving weights 4\n",
      "41 d0:0.24899503588676453 d1:0.2121581733226776   g:5.51155424118042\n",
      "42 d0:0.298229843378067 d1:0.2025301158428192   g:4.171195030212402\n",
      "43 d0:0.4859066903591156 d1:0.23104001581668854   g:6.8655290603637695\n",
      "44 d0:0.23363491892814636 d1:0.301075279712677   g:3.7330446243286133\n",
      "45 d0:0.7888813018798828 d1:0.28275489807128906   g:8.824508666992188\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-5091611102f3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmkdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0manim_dir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mtrain_gan\u001b[0m\u001b[0;34m(\u001b[0m \u001b[0;34m\"data.hdf5\"\u001b[0m \u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-7-85208eead929>\u001b[0m in \u001b[0;36mtrain_gan\u001b[0;34m(dataf)\u001b[0m\n\u001b[1;32m    138\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mh5py\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mFile\u001b[0m\u001b[0;34m(\u001b[0m \u001b[0mdataf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'r'\u001b[0m \u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    139\u001b[0m         \u001b[0mpokemons\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m \u001b[0;34m'pokemons'\u001b[0m \u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 140\u001b[0;31m         \u001b[0mrun_batches\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgen\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdisc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgan\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpokemons\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogger\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m5000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    141\u001b[0m     \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    142\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-7-85208eead929>\u001b[0m in \u001b[0;36mrun_batches\u001b[0;34m(gen, disc, gan, pokemons, logger, itr_generator)\u001b[0m\n\u001b[1;32m    162\u001b[0m         \u001b[0mgen\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrainable\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    163\u001b[0m         \u001b[0;31m#for reals, fakes in history:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 164\u001b[0;31m         \u001b[0md_loss1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdisc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_on_batch\u001b[0m\u001b[0;34m(\u001b[0m \u001b[0mreals\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlbl_real\u001b[0m \u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    165\u001b[0m         \u001b[0md_loss0\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdisc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_on_batch\u001b[0m\u001b[0;34m(\u001b[0m \u001b[0mfakes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlbl_fake\u001b[0m \u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    166\u001b[0m         \u001b[0mgen\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrainable\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/git/PokeGAN/env/lib/python3.5/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mtrain_on_batch\u001b[0;34m(self, x, y, sample_weight, class_weight)\u001b[0m\n\u001b[1;32m   1535\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1536\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_train_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1537\u001b[0;31m       \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1538\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1539\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/git/PokeGAN/env/lib/python3.5/site-packages/tensorflow/python/keras/backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2895\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_callable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeed_arrays\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_symbols\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msymbol_vals\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msession\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2896\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2897\u001b[0;31m     \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2898\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mfetched\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2899\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/git/PokeGAN/env/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1449\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_created_with_new_api\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1450\u001b[0m           return tf_session.TF_SessionRunCallable(\n\u001b[0;32m-> 1451\u001b[0;31m               self._session._session, self._handle, args, status, None)\n\u001b[0m\u001b[1;32m   1452\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1453\u001b[0m           return tf_session.TF_DeprecatedSessionRunCallable(\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "if not os.path.exists(snapshot_dir):\n",
    "    os.mkdir(snapshot_dir)\n",
    "if not os.path.exists(anim_dir):\n",
    "    os.mkdir(anim_dir)\n",
    "\n",
    "train_gan( \"data.hdf5\" )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
